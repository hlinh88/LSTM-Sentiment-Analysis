{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from textblob import TextBlob\n",
    "import math\n",
    "from collections import Counter, defaultdict\n",
    "import tweepy as tw\n",
    "import json\n",
    "import pandas as pd\n",
    "import re\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from bs4 import BeautifulSoup\n",
    "%matplotlib inline\n",
    "import nltk\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.stem import WordNetLemmatizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "api_key = \"HAcIeikl6eRheQP1oKYGDwPx0\"\n",
    "api_secret = \"gEWmqG7QVXzudnviUXDo98L2UKGYg9PNqy1bAR1geClkHjqBGk\"\n",
    "access_token = \"1481095074465579008-1pApYlU4HRJWpbAXcbdVEVGzdACuEz\"\n",
    "access_token_secret = \"htgD81H4mOxXw5MglAljFRHhcfGAuwsNomXVqvGw0Obwe\"\n",
    "auth = tw.OAuthHandler(api_key, api_secret)\n",
    "auth.set_access_token(access_token, access_token_secret)\n",
    "api = tw.API(auth,wait_on_rate_limit=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "is_executing": true,
     "name": "#%%\n"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Amazon News\n"
     ]
    }
   ],
   "source": [
    "number_of_tweets = 500\n",
    "tweets = []\n",
    "user_id =[]\n",
    "created_at = []\n",
    "keyword_list = ['News', 'Stock'] # , 'Customers', 'Employees', 'Company', 'Price', 'Shareholders', 'Market', 'Soaring', 'Trends'\n",
    "company = 'Amazon'\n",
    "final_kw_list = []\n",
    "\n",
    "# Crawl data from keywords\n",
    "for i in range (len(keyword_list)):\n",
    "    kw = str(company + ' ' + keyword_list[i])\n",
    "    print(kw)\n",
    "    final_kw_list.append(kw)\n",
    "    for i in tw.Cursor(api.search, q = kw , lang =\"en\", since = \"2022-01-14\",tweet_mode = \"extended\").items(number_of_tweets):\n",
    "        tweets.append(i.full_text)\n",
    "        user_id.append(i.user.id)\n",
    "        created_at.append(i.created_at)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "is_executing": true,
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "df = pd.DataFrame({\"Created_at\": created_at,\"User_id\": user_id,\"Tweets\":tweets})\n",
    "df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "is_executing": true,
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "df = df[~df.Tweets.str.contains(\"RT\")]\n",
    "df = df.reset_index(drop = True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "is_executing": true,
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "def clean_up_tweet(text):\n",
    "    text = re.sub(r'@[A-Za-z0-9_]+','',text) # Remove mentions @\n",
    "    text = re.sub(r'#', '', text) # Remove hastag symbol\n",
    "    text = re.sub(r'https?:\\/\\/[A-Za-z0-9\\.\\/]+', '', text) # Remove hyper link\n",
    "    text = re.sub('[^A-Za-z0-9]+', ' ', text) # Remove special characters\n",
    "    text = re.sub(r'[0-9]+', '', text)\n",
    "    #Remove stop words\n",
    "    res = ''\n",
    "    for word in text.split():\n",
    "        if word not in stopwords.words('english'):\n",
    "            res += WordNetLemmatizer().lemmatize(word) + ' '\n",
    "\n",
    "    return res"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "is_executing": true,
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "df['Tweets'] = df['Tweets'].apply(clean_up_tweet)\n",
    "df = df.drop_duplicates(subset='Tweets')\n",
    "df = df.reset_index(drop=True)\n",
    "df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "is_executing": true,
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "# Create function to get subjectivity\n",
    "def getSubjectivity(text):\n",
    "    return TextBlob(text).sentiment.subjectivity\n",
    "\n",
    "# Create function to get polarity\n",
    "def getPolarity(text):\n",
    "    return TextBlob(text).sentiment.polarity\n",
    "\n",
    "# Function compute negative, positive\n",
    "df['Subjectivity'] = df['Tweets'].apply(getSubjectivity)\n",
    "df['Polarity'] = df['Tweets'].apply(getPolarity)\n",
    "\n",
    "df = df.reset_index(drop=True)\n",
    "df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "is_executing": true,
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "from nltk.corpus import stopwords\n",
    "stoplist = stopwords.words('english')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "is_executing": true,
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "c_vec = CountVectorizer(stop_words=stoplist, ngram_range=(2,3))\n",
    "# matrix of n_grams\n",
    "ngrams = c_vec.fit_transform(df['Tweets'])\n",
    "# count frequency of ngrams\n",
    "count_values = ngrams.toarray().sum(axis=0)\n",
    "# list of ngrams\n",
    "vocab = c_vec.vocabulary_\n",
    "df_ngram = pd.DataFrame(sorted([(count_values[i],k) for k,i in vocab.items()], reverse=True)\n",
    "            ).rename(columns={0: 'Frequency', 1:'n-gram'})\n",
    "\n",
    "df_ngram = df_ngram.drop_duplicates(subset='n-gram')\n",
    "df_ngram = df_ngram.reset_index(drop=True)\n",
    "df_ngram['Polarity'] = df_ngram['n-gram'].apply(lambda x: TextBlob(x).polarity)\n",
    "df_ngram['Subjective'] = df_ngram['n-gram'].apply(lambda x: TextBlob(x).subjectivity)\n",
    "df_ngram.drop(df_ngram.loc[df_ngram['Polarity'] == 0.0].index, inplace=True)\n",
    "df_ngram = df_ngram.reset_index(drop=True)\n",
    "df_ngram"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "is_executing": true,
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import f1_score, accuracy_score\n",
    "\n",
    "y = df['Polarity'].values.astype('float')\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(df[\"Reviews\"], y, random_state=42, test_size=0.2)\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "is_executing": true,
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "vect = CountVectorizer(min_df= 5, ngram_range=(2,3)).fit(X_train)\n",
    "X_train_vectorized = vect.transform(X_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "is_executing": true,
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "from sklearn.linear_model import LogisticRegression\n",
    "model = LogisticRegression()\n",
    "model.fit(X_train_vectorized, y_train)\n",
    "predictions = model.predict(vect.transform(X_test))\n",
    "print(predictions)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "is_executing": true,
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "# from sklearn.metrics import classification_report\n",
    "# import numpy as np\n",
    "# from sklearn import linear_model\n",
    "# from sklearn import svm\n",
    "#\n",
    "# classifiers = [\n",
    "#     svm.SVR(),\n",
    "#     linear_model.SGDRegressor(),\n",
    "#     linear_model.LinearRegression()]\n",
    "#\n",
    "# trainingData    = X_train\n",
    "# trainingScores  = y_train\n",
    "# predictionData  = X_test\n",
    "#\n",
    "# for item in classifiers:\n",
    "#     print(item)\n",
    "#     clf = item\n",
    "#     clf.fit(trainingData, trainingScores)\n",
    "#     print(y_test)\n",
    "#     print(clf.predict(predictionData),'\\n')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}